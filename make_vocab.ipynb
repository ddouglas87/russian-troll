{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make vocabulary file\n",
    "\n",
    "The vocabulary file Albert comes with is trained on Wikipedia, which will not have words like MAGA or other tweet slang.  So, let's train our own twitter vocabulary file here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2116866, 21)\n",
      "Come support our peace officers. Our choice is law & order. #BlueLivesMatter  Rally link:  https://t.co/PuKr7nBBxP https://t.co/QpCg8jRFkN\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['external_author_id', 'author', 'content', 'region', 'language',\n",
       "       'publish_date', 'harvested_date', 'following', 'followers', 'updates',\n",
       "       'post_type', 'account_type', 'retweet', 'account_category',\n",
       "       'new_june_2018', 'alt_external_id', 'tweet_id', 'article_url',\n",
       "       'tco1_step1', 'tco2_step1', 'tco3_step1'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Russian Troll Tweets dataset\n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "folder = \"data/russian-troll-tweets-master\"\n",
    "russian_tweets = pd.concat([pd.read_csv(file, dtype='str') for file in glob.glob(folder + '/*.csv')])\n",
    "\n",
    "russian_tweets = russian_tweets[russian_tweets['language'] == 'English']  # English tweets only.\n",
    "russian_tweets = russian_tweets[pd.notnull(russian_tweets['content'])]  # drop null tweets\n",
    "\n",
    "print(russian_tweets.shape)\n",
    "print(russian_tweets[russian_tweets['account_category'] == 'RightTroll'].iloc[0]['content'])\n",
    "russian_tweets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 37)\n",
      "100000\n",
      "RT @beastieboys: ‚ÄúWait, what!?! I just heard that Mike &amp; Adam made a movie about Beastie Boys with Spike Jonze. Is that for real?\" - Larry‚Ä¶\n",
      "Index(['created_at', 'id', 'id_str', 'text', 'source', 'truncated',\n",
      "       'in_reply_to_status_id', 'in_reply_to_status_id_str',\n",
      "       'in_reply_to_user_id', 'in_reply_to_user_id_str',\n",
      "       'in_reply_to_screen_name', 'user', 'geo', 'coordinates', 'place',\n",
      "       'contributors', 'retweeted_status', 'is_quote_status', 'quote_count',\n",
      "       'reply_count', 'retweet_count', 'favorite_count', 'entities',\n",
      "       'favorited', 'retweeted', 'filter_level', 'lang', 'timestamp_ms',\n",
      "       'display_text_range', 'quoted_status_id', 'quoted_status_id_str',\n",
      "       'quoted_status', 'quoted_status_permalink', 'possibly_sensitive',\n",
      "       'extended_tweet', 'extended_entities', 'withheld_in_countries'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Import Twitter Stream dataset\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "stream_tweets = pd.read_json(\"data/en_tweets.json\", dtype='str')\n",
    "\n",
    "print(stream_tweets.shape)\n",
    "print(stream_tweets[stream_tweets['lang'] == 'en'].count()[0])\n",
    "print(stream_tweets['text'][0])\n",
    "stream_tweets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean tweets / normalize tweets\n",
    "# Sentencepiece states \"one-sentence-per-line raw corpus file. No need to run tokenizer, normalizer or preprocessor.\"\n",
    "\n",
    "# Replace newlines with spaces, because sentencepiece does not like newlines.\n",
    "russian_tweets['content'].replace('\\n', ' ', regex=True, inplace=True)\n",
    "stream_tweets['text'].replace('\\n', ' ', regex=True, inplace=True)\n",
    "\n",
    "# Lowercase all the tweets\n",
    "russian_tweets['content'] = russian_tweets['content'].str.lower()\n",
    "stream_tweets['text'] = stream_tweets['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2216866\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['police: airline pilot found passed out in cockpit was drunk https://t.co/gcqwioeswj https://t.co/2agimsvzmk',\n",
       "       'dashcam video shows man launching himself onto police cruiser https://t.co/2zgpbau4ey https://t.co/b2fknfpy5v',\n",
       "       'man arrested for setting fire to south sf medical clinic new year‚Äôs day https://t.co/maqc2gzvhv https://t.co/wgxyh0ifto',\n",
       "       ..., 'rt @more_milf: the music, the crowd! she stole the show!',\n",
       "       '@itaintwhiteboy @chadjohnwallis i used to like him in the past, i thought that he was a pretty normal bernie bros..‚Ä¶ https://t.co/wg4fav3tlg',\n",
       "       '@elise_flowers always my queen always ‚ù§üôèüèæ you take care ‚ù§'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create combined dataset\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# We only need the tweet text for this.\n",
    "combined_tweets = pd.concat([russian_tweets['content'], stream_tweets['text']]).values\n",
    "\n",
    "print(len(combined_tweets))\n",
    "combined_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to file in format SentencePiece expects.\n",
    "\n",
    "with open(\"data/vocab/sentences.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    for t in combined_tweets:\n",
    "        file.write(t+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run Sentencepiece to make the vocab file for Albert\n",
    "\n",
    "# !pip3 install sentencepiece\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "# Settings grabbed from https://github.com/google-research/ALBERT#sentencepiece\n",
    "spm_settings = r'''\n",
    "--input=data/vocab/sentences.txt --model_prefix=data/vocab/twitter-30k --vocab_size=30000\n",
    "--pad_id=0 --unk_id=1 --eos_id=-1 --bos_id=-1 --control_symbols=[CLS],[SEP],[MASK]\n",
    "--user_defined_symbols=(,),\\‚Äù,-,.,‚Äì,¬£,‚Ç¨ --input_sentence_size=10000000\n",
    "--character_coverage=0.99995 --model_type=unigram\n",
    "'''\n",
    "spm_settings = spm_settings.replace('\\n',' ')\n",
    "\n",
    "spm.SentencePieceTrainer.Train(spm_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check vocab files\n",
    "\n",
    "import random\n",
    "\n",
    "def read_sentencepiece_vocab(filepath):\n",
    "  voc = []\n",
    "  with open(filepath, encoding='utf-8') as fi:\n",
    "    for line in fi:\n",
    "      voc.append(line.split(\"\\t\")[0])\n",
    "  # skip the first <unk> token\n",
    "  voc = voc[1:]\n",
    "  return voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned vocab size: 29999\n",
      "Sample tokens: ['pkk', '‚ñÅindonesian', 'my', 'span', 'kpxt', 'nka', 'jcz', '‚ñÅapologist', '‚ñÅwives', '‚ñÅarrest']\n"
     ]
    }
   ],
   "source": [
    "# Check custom Twitter vocab\n",
    "\n",
    "twitter_vocab = read_sentencepiece_vocab(\"data/vocab/twitter-30k.vocab\")\n",
    "print(\"Learned vocab size: {}\".format(len(twitter_vocab)))\n",
    "random.seed(124)\n",
    "print(\"Sample tokens: {}\".format(random.sample(twitter_vocab, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned vocab size: 29999\n",
      "Sample tokens: ['‚ñÅrepresent', '‚ñÅtruss', '‚ñÅpractised', '‚ñÅinteracting', '‚ñÅpare', '‚ñÅguiding', '‚ñÅfen', 'hoo', '‚ñÅbelievers', '‚ñÅstanford']\n"
     ]
    }
   ],
   "source": [
    "# Check vocab that came with Albert\n",
    "\n",
    "albert_vocab = read_sentencepiece_vocab(\"model/assets/30k-clean.vocab\")\n",
    "print(\"Learned vocab size: {}\".format(len(albert_vocab)))\n",
    "random.seed(14)\n",
    "print(\"Sample tokens: {}\".format(random.sample(albert_vocab, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
