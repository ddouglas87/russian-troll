{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Datset\n",
    "\n",
    "Converts Russian Troll Bot dataset to train, test, and validation tsv files.  Albert expects this format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2116866, 21)\n",
      "Come support our peace officers. Our choice is law & order. #BlueLivesMatter  Rally link:  https://t.co/PuKr7nBBxP https://t.co/QpCg8jRFkN\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['external_author_id', 'author', 'content', 'region', 'language',\n",
       "       'publish_date', 'harvested_date', 'following', 'followers', 'updates',\n",
       "       'post_type', 'account_type', 'retweet', 'account_category',\n",
       "       'new_june_2018', 'alt_external_id', 'tweet_id', 'article_url',\n",
       "       'tco1_step1', 'tco2_step1', 'tco3_step1'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Russian Troll Tweets dataset\n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "folder = \"data/russian-troll-tweets-master\"\n",
    "russian_tweets = pd.concat([pd.read_csv(file, dtype='str') for file in glob.glob(folder + '/*.csv')])\n",
    "\n",
    "russian_tweets = russian_tweets[russian_tweets['language'] == 'English']  # English tweets only.\n",
    "russian_tweets = russian_tweets[pd.notnull(russian_tweets['content'])]  # drop null tweets\n",
    "\n",
    "russian_tweets['publish_date'] = pd.to_datetime(russian_tweets['publish_date'])\n",
    "\n",
    "print(russian_tweets.shape)\n",
    "print(russian_tweets[russian_tweets['account_category'] == 'RightTroll'].iloc[0]['content'])\n",
    "russian_tweets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create date range dataframes, for comparing model training performance by year.\n",
    "\n",
    "rt2015 = russian_tweets[russian_tweets['publish_date'].dt.year == 2015]\n",
    "rt2016 = russian_tweets[russian_tweets['publish_date'].dt.year == 2016]\n",
    "rt2017 = russian_tweets[russian_tweets['publish_date'].dt.year == 2017]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 37)\n",
      "100000\n",
      "RT @beastieboys: ‚ÄúWait, what!?! I just heard that Mike &amp; Adam made a movie about Beastie Boys with Spike Jonze. Is that for real?\" - Larry‚Ä¶\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['created_at', 'id', 'id_str', 'text', 'source', 'truncated',\n",
       "       'in_reply_to_status_id', 'in_reply_to_status_id_str',\n",
       "       'in_reply_to_user_id', 'in_reply_to_user_id_str',\n",
       "       'in_reply_to_screen_name', 'user', 'geo', 'coordinates', 'place',\n",
       "       'contributors', 'retweeted_status', 'is_quote_status', 'quote_count',\n",
       "       'reply_count', 'retweet_count', 'favorite_count', 'entities',\n",
       "       'favorited', 'retweeted', 'filter_level', 'lang', 'timestamp_ms',\n",
       "       'display_text_range', 'quoted_status_id', 'quoted_status_id_str',\n",
       "       'quoted_status', 'quoted_status_permalink', 'possibly_sensitive',\n",
       "       'extended_tweet', 'extended_entities', 'withheld_in_countries'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Twitter Stream dataset\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "stream_tweets = pd.read_json(\"data/en_tweets.json\", dtype='str')\n",
    "\n",
    "# Replace newlines and tabs with spaces, because output is a tsv.\n",
    "stream_tweets['text'].replace('\\t', ' ', regex=True, inplace=True)\n",
    "stream_tweets['text'].replace('\\n', ' ', regex=True, inplace=True)\n",
    "\n",
    "# Convert str type to date type.\n",
    "stream_tweets['created_at'] = pd.to_datetime(stream_tweets['created_at'])\n",
    "\n",
    "print(stream_tweets.shape)\n",
    "print(stream_tweets[stream_tweets['lang'] == 'en'].count()[0])\n",
    "print(stream_tweets['text'][0])\n",
    "stream_tweets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000\n",
      "45000\n",
      "45000\n",
      "45000\n",
      "45000\n"
     ]
    }
   ],
   "source": [
    "# Create Train and Test Sets\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Limit number of samples to keep training fast\n",
    "TOTAL_NUM_SAMPLES = 100_000\n",
    "CATEGORIES = 4\n",
    "numb_samples = int(TOTAL_NUM_SAMPLES/CATEGORIES)\n",
    "\n",
    "# Column names Albert expects\n",
    "X_COLUMN = \"text\"\n",
    "Y_COLUMN = \"label\"\n",
    "\n",
    "# Create non-troll train,test,validate dataset\n",
    "def create_nontroll_train_test_validate_sets(df):\n",
    "    st = pd.DataFrame({X_COLUMN: df['text'], Y_COLUMN: pd.DataFrame(['NotTroll']*len(df['text']))[0]})\n",
    "    st = st.sample(numb_samples)  # Keep training size small and evenly distributed.\n",
    "    st.loc[st[Y_COLUMN] == 'NotTroll',Y_COLUMN] = 3  # Albert expects category names to be numbers.\n",
    "    \n",
    "    # Split data into train and test sets\n",
    "    # 60% train, 20% validation, 20% test\n",
    "    train, validate, test = np.split(st.sample(frac=1), [int(0.6*len(st)), int(0.8*len(st))])\n",
    "    return train, validate, test\n",
    "\n",
    "def create_russian_troll_train_test_validate_sets(df):\n",
    "    \"\"\"Used\"\"\"\n",
    "    rt = pd.DataFrame({X_COLUMN: df['content'], Y_COLUMN: df['account_category']})\n",
    "    \n",
    "    # Select only trolls and news feeds\n",
    "    left_troll = rt[rt[Y_COLUMN] == 'LeftTroll'].sample(numb_samples)\n",
    "    right_troll = rt[rt[Y_COLUMN] == 'RightTroll'].sample(numb_samples)\n",
    "    news_feed = rt[rt[Y_COLUMN] == 'NewsFeed'].sample(numb_samples)\n",
    "    rt = pd.concat([left_troll, right_troll, news_feed])\n",
    "    \n",
    "    # Convert categories to ints. Albert expects ints.\n",
    "    rt.loc[rt[Y_COLUMN] == 'LeftTroll',Y_COLUMN] = 0\n",
    "    rt.loc[rt[Y_COLUMN] == 'RightTroll',Y_COLUMN] = 1\n",
    "    rt.loc[rt[Y_COLUMN] == 'NewsFeed',Y_COLUMN] = 2\n",
    "    \n",
    "    # Split data into train and test sets\n",
    "    # 60% train, 20% validation, 20% test\n",
    "    train, validate, test = np.split(rt.sample(frac=1), [int(0.6*len(rt)), int(0.8*len(rt))])\n",
    "    return train, validate, test\n",
    "\n",
    "\n",
    "st_train, st_validate, st_test = create_nontroll_train_test_validate_sets(stream_tweets)\n",
    "print(len(st_train))\n",
    "\n",
    "rt_train, rt_validate, rt_test = create_russian_troll_train_test_validate_sets(russian_tweets)\n",
    "print(len(rt_train))\n",
    "\n",
    "rt2015_train, rt2015_validate, rt2015_test = create_russian_troll_train_test_validate_sets(rt2015)\n",
    "rt2016_train, rt2016_validate, rt2016_test = create_russian_troll_train_test_validate_sets(rt2016)\n",
    "rt2017_train, rt2017_validate, rt2017_test = create_russian_troll_train_test_validate_sets(rt2017)\n",
    "print(len(rt2015_train))\n",
    "print(len(rt2016_train))\n",
    "print(len(rt2017_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to tab seperate value files.\n",
    "\n",
    "from pathlib import Path\n",
    "import csv\n",
    "\n",
    "def write_to_tsv(df, path, filename):\n",
    "    path = Path(path)\n",
    "    \n",
    "    # Make folder if it doesn't already exist.\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    df.to_csv(str(path/filename)+'.tsv', sep='\\t', index=False)\n",
    "\n",
    "def write_train_validate_test_to_tsv(train, validate, test, path):\n",
    "    # Shuffle up tweets, as Albert may read this in order, causing faulty training.\n",
    "    train = train.sample(frac=1)\n",
    "    validate = validate.sample(frac=1)\n",
    "    test = test.sample(frac=1)\n",
    "    \n",
    "    write_to_tsv(train, path, \"train\")\n",
    "    write_to_tsv(validate, path, \"dev\")\n",
    "    write_to_tsv(test, path, \"test\")\n",
    "\n",
    "    \n",
    "# All years tweets (2015-2017)\n",
    "write_train_validate_test_to_tsv(pd.concat([rt_train, st_train], ignore_index=True),\n",
    "                                 pd.concat([rt_validate, st_validate], ignore_index=True),\n",
    "                                 pd.concat([rt_test, st_test], ignore_index=True),\n",
    "                                 'data/tweets')\n",
    "\n",
    "# Trained on 2015 tweets, validated on 2015 tweets\n",
    "write_train_validate_test_to_tsv(pd.concat([rt2015_train, st_train], ignore_index=True),\n",
    "                                 pd.concat([rt2015_validate, st_validate], ignore_index=True),\n",
    "                                 pd.concat([rt2015_test, st_test], ignore_index=True),\n",
    "                                 'data/2015/tweets')\n",
    "\n",
    "# Trained on 2015 tweets, validated on 2016 tweets\n",
    "write_train_validate_test_to_tsv(pd.concat([rt2015_train, st_train], ignore_index=True),\n",
    "                                 pd.concat([rt2016_validate, st_validate], ignore_index=True),\n",
    "                                 pd.concat([rt2016_test, st_test], ignore_index=True),\n",
    "                                 'data/2016/tweets')\n",
    "\n",
    "# Trained on 2015 tweets, validated on 2017 tweets\n",
    "write_train_validate_test_to_tsv(pd.concat([rt2015_train, st_train], ignore_index=True),\n",
    "                                 pd.concat([rt2017_validate, st_validate], ignore_index=True),\n",
    "                                 pd.concat([rt2017_test, st_test], ignore_index=True),\n",
    "                                 'data/2017/tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Nationals' Cole suspended 5 games for throwing at Kang  #sports</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RT Chet_Cannon: 'Gay sex-worker, 26, dies of meth overdose at Hollywood home of high-profile Democrat donor' ‚Ä¶ https://t.co/WtVAFIiJRV</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>It's officially \"nah I'm good weather\". So if you invite me somewhere, just know I'm good lol</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rape Survivor Explains Why She Supports The Second Amendment [VIDEO]  https://t.co/N6jxQsH59j</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chitown in DC.      #westside https://t.co/d3hCWCuI6Q</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>That`s really funny http://t.co/voqXc623rY</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Here Are Six GOP TRAITOR Senators Who Need To GO https://t.co/7hUwRQ3nSR https://t.co/mXGP5gYQe9</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NYC ‚úàÔ∏è https://t.co/2ZukGPIX9Y</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PD: Person shot, killed near Findlay Playground https://t.co/odePF7CNR8 https://t.co/hYlzyi2zZM</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RT @Amonofhell: #AsimRiaz saying, ‚ÄúBB please give us some time‚Äù again and again broke my mfking heart üòîüíîüíîüíî he was trying so hard to explain‚Ä¶</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60001 rows √ó 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   label\n",
       "text                                                    \n",
       "Nationals' Cole suspended 5 games for throwing ...     2\n",
       "RT Chet_Cannon: 'Gay sex-worker, 26, dies of me...     1\n",
       "It's officially \"nah I'm good weather\". So if y...     0\n",
       "Rape Survivor Explains Why She Supports The Sec...     0\n",
       "Chitown in DC.      #westside https://t.co/d3hC...     0\n",
       "...                                                  ...\n",
       "That`s really funny http://t.co/voqXc623rY             0\n",
       "Here Are Six GOP TRAITOR Senators Who Need To G...     1\n",
       "NYC ‚úàÔ∏è https://t.co/2ZukGPIX9Y                         0\n",
       "PD: Person shot, killed near Findlay Playground...     2\n",
       "RT @Amonofhell: #AsimRiaz saying, ‚ÄúBB please gi...     3\n",
       "\n",
       "[60001 rows x 1 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "def read_tsv(path, filename):\n",
    "    return pd.read_csv(str(Path(path)/filename)+'.tsv', sep='\\t', index_col=0, dtype='str')\n",
    "\n",
    "read_tsv(\"data/tweets\", \"train\")\n",
    "# read_tsv(\"data/2015/tweets\", \"train\")\n",
    "# read_tsv(\"data/2015/tweets\", \"dev\")\n",
    "# read_tsv(\"data/2015/tweets\", \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
